# Voice Evaluation Benchmarks

This document is a versioned benchmark snapshot for voice recognition quality and performance.

## Snapshot

- Generated at (UTC): `2026-02-13T19:59:17Z`
- Dataset: `tests/voice_eval/manifest.json`
- Samples: `10` (Japanese, single-speaker recordings)
- Hardware: Apple Silicon (`M4 Max`)
- Local raw result source: `tests/voice_eval/model-comparison.json` (git-ignored)

## ASR Model Results

| Model | Runtime | WER | CER | Avg latency (ms) | Avg RTF |
| --- | --- | ---: | ---: | ---: | ---: |
| `whisper/tiny` | `whisper-rs` | 0.8462 | 0.1682 | 154.8 | 0.0308 |
| `whisper/base` | `whisper-rs` | 1.0769 | 0.0773 | 337.8 | 0.0683 |
| `whisper/small` | `whisper-rs` | 0.6154 | 0.0591 | 769.3 | 0.1537 |
| `whisper/medium-q5_0` | `whisper-rs` | 0.6154 | 0.0318 | 1725.0 | 0.3447 |
| `whisper/large-v3-turbo-q5_0` | `whisper-rs` | 0.3846 | 0.0045 | 2287.0 | 0.4574 |
| `whisper/large-v3-q5_0` | `whisper-rs` | 0.4615 | 0.0136 | 3103.1 | 0.6195 |
| `Qwen/Qwen3-ASR-0.6B` | `qwen-asr + transformers` | 0.5385 | 0.0364 | 461.0 | 0.0888 |
| `Qwen/Qwen3-ASR-1.7B` | `qwen-asr + transformers` | 0.3846 | 0.0091 | 443.0 | 0.0868 |
| `microsoft/VibeVoice-ASR` | `vibevoice + transformers` | 0.5385 | 0.0364 | 2176.9 | 0.4318 |

## Download Benchmarks

| Model | Size (MiB) | Elapsed (s) | Speed (MiB/s) | Method |
| --- | ---: | ---: | ---: | --- |
| `Qwen/Qwen3-ASR-0.6B` | 1789.2 | 39.69 | 45.08 | `hf_hub_download force_download` |
| `Qwen/Qwen3-ASR-1.7B` | 4480.9 | 102.18 | 43.85 | `hf_hub_download force_download` |
| `whisper/medium-q5_0` | 514.2 | 14.85 | 34.62 | `curl direct download` |
| `whisper/large-v3-turbo-q5_0` | 547.4 | 15.68 | 34.91 | `curl direct download` |
| `whisper/large-v3-q5_0` | 1031.1 | 29.63 | 34.80 | `curl direct download` |
| `microsoft/VibeVoice-ASR` (single shard) | 1039.5 | 27.86 | 37.31 | `hf_hub_download force_download` |
| `microsoft/VibeVoice-ASR` (total, estimated) | 16544.5 | 443.44 | 37.31 | estimated from `model.safetensors.index.json` |

## Notes

- Whisper metrics come from `tests/voice_eval/latest-report.json` generated by `scripts/voice-eval.sh`.
- Qwen/VibeVoice metrics were measured with manual Python runs using the same manifest and normalization policy.
- `VibeVoice-ASR` evaluation used concatenated text content for WER/CER (speaker/timestamp metadata was excluded from scoring).

## Update Process

1. Refresh whisper metrics with `scripts/voice-eval.sh`.
1. Refresh cross-model metrics into `tests/voice_eval/model-comparison.json`.
1. Update this document with the new snapshot timestamp and table values.
